"""
COnverts an SQL file with INSERT statements to CSV for easier cross-DB imports.
Should work with any SQL dialect (T-SQL, MySQL, PostgreSQL, etc.).
BUT has only really been tested with T-SQL sor far
"""

import csv
import logging
from pathlib import Path

from sqlglot import exp, parse


def extract_value(val: exp.Expression) -> str | int | float | None:
    """Extracts the python value of an SQLGlot Expression. Defaults to str"""
    if isinstance(val, exp.Null):
        return None
    elif isinstance(val, exp.National):
        return val.this
    elif isinstance(val, exp.Literal):
        if val.is_int:
            return int(val.this)
        elif val.is_number:
            return float(val.this)
        return val.this
    else:
        return str(val)


def sql_to_csv(
    sql_path: Path,
    output_path: Path | None = None,
    encoding: str = "iso-8859-1", # This is the encoding of the CODEX extracts
    dialect: str = "tsql", # This is the SQL format of the CODEX extracts
) -> dict[str, int]:
    """
    Converts a SQL file to CSV

    Args:
        sql_path: path to the SQL file
        output_path: output CSV path (default: same place, same name with .csv)
        encoding: SQL file's encoding
        dialect: source SQL's dialect (tsql, mysql, postgres, etc.)

    Returns:
        Dict with stats: {'table': table_name, 'rows': nb_lines, 'columns': nb_columns}
    """
    with open(sql_path, encoding=encoding) as f:
        content = f.read()

    statements = parse(content, dialect=dialect)

    # Extract data
    table_name = None
    columns = []
    rows = []

    for stmt in statements:
        if not isinstance(stmt, exp.Insert):
            logging.warning(f"This SQL file has non-insert statements of type {type(stmt)}. Some information could be lost in the process.")
            continue

        # Table name (stmt.this is a Schema, the actual Table is in stmt.this.this)
        if table_name is None and stmt.this and stmt.this.this:
            table_name = stmt.this.this.name

        # Columns (from the INSERT schema)
        if not columns and stmt.this and hasattr(stmt.this, "expressions"):
            columns = [col.name for col in stmt.this.expressions]

        # Values
        for values_node in stmt.find_all(exp.Values):
            for row_expr in values_node.expressions:
                row_values = [extract_value(v) for v in row_expr.expressions]
                rows.append(row_values)

    if not rows:
        logging.warning(f"No INSERTABLE data in {sql_path}")
        return {"table": None, "rows": 0, "columns": 0}

    # CSV output
    if output_path is None:
        output_path = sql_path.with_suffix(".csv")

    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)

        # use detected columns to generate columns names
        # else, create autogenerated columns
        if columns:
            writer.writerow(columns)
        else:
            writer.writerow([f"col{i}" for i in range(len(rows[0]))])

        # write data
        for row in rows:
            writer.writerow(["" if v is None else v for v in row])

    logging.info(f"Exports: {output_path} ({len(rows)} lines, {len(columns) or len(rows[0])} columns)")

    return {
        "table": table_name,
        "rows": len(rows),
        "columns": len(columns) or len(rows[0]),
    }